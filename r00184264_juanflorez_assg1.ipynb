{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "r00184264_juanflorez_assg1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqhnpw8B5/1lt4CLr30NvU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanflorezVe/COMP9058_lab1/blob/master/r00184264_juanflorez_assg1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJivgFpXEloh"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl9KfcjsB09Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d36ba953-f314-4b7c-bd7e-58f39e7e9acb"
      },
      "source": [
        "# Copying vervatim the code from the assignment\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "experiment = tf.Variable([1,2,3])\n",
        "print(tf.divide(tf.reduce_sum(experiment),experiment.shape[0]))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "tf.Tensor(2.0, shape=(), dtype=float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1I7TLooA7uG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41244e48-f45b-4890-dd3a-b33c715b30e6"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from keras.utils import np_utils\n",
        "\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "# load the training and test data    \n",
        "(tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
        "\n",
        "# reshape the feature data\n",
        "tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
        "te_x = te_x.reshape(te_x.shape[0], 784)\n",
        "\n",
        "# noramlise feature data\n",
        "tr_x = tr_x / 255.0\n",
        "te_x = te_x / 255.0\n",
        "\n",
        "print( \"Shape of training features \", tr_x.shape)\n",
        "print( \"Shape of test features \", te_x.shape)\n",
        "\n",
        "\n",
        "# one hot encode the training labels and get the transpose\n",
        "tr_y = np_utils.to_categorical(tr_y,10)\n",
        "tr_y = tr_y.T\n",
        "print (\"Shape of training labels \", tr_y.shape)\n",
        "\n",
        "# one hot encode the test labels and get the transpose\n",
        "te_y = np_utils.to_categorical(te_y,10)\n",
        "te_y = te_y.T\n",
        "print (\"Shape of testing labels \", te_y.shape)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of training features  (60000, 784)\n",
            "Shape of test features  (10000, 784)\n",
            "Shape of training labels  (10, 60000)\n",
            "Shape of testing labels  (10, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLfLKaoqUF2e"
      },
      "source": [
        "#Let's insert 784 parameters to each neuron\n",
        "# A = W.T * X + b\n",
        "# layer_pass will take the x matrix, pass it to the l layer applying the w weights and adding the b bias\n",
        "#NeuralNetworksW5 slide 11\n",
        "# m = 60000\n",
        "# n = 784\n",
        "# p = 200\n",
        "def relu_layer_pass(w, x, b):\n",
        "  t = tf.matmul(w,x)\n",
        "  l = tf.add(t,b)\n",
        "  h = tf.nn.relu(l)\n",
        "  return h\n",
        "\n",
        "def softmax_layer_pass(w, x, b):\n",
        "  t = tf.matmul(w,x)\n",
        "  l = tf.add(t,b)\n",
        "  l_exp = tf.math.exp(l)\n",
        "  sums = tf.reduce_sum(l_exp,0)\n",
        "  h = l_exp/sums\n",
        "\n",
        "  return h\n",
        "\n",
        "#TODO: X is the matrix with the training instances,\n",
        "#but I need a function that given an Error, E, and a\n",
        "# vector with inputs, gives me the d_lambda and db\n",
        "\n",
        "def loss_function(E, x=None):\n",
        "  db = tf.divide(tf.reduce_sum(E),E.shape[0])\n",
        "  print(\"shape of x {}, shape of E transposed {}\".\n",
        "        format(x.shape, tf.transpose(E).shape))\n",
        "  temp = tf.matmul(x, tf.transpose(E))\n",
        "  d_lambda = tf.divide(temp, E.shape[0])\n",
        "  return db, d_lambda"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHo4xMShUmHX"
      },
      "source": [
        "def forward_pass(w1, w2, b1, b2, x):\n",
        "  #pass the first layer\n",
        "  h1 = relu_layer_pass(w1,x,b1)\n",
        "  #pass the second layer\n",
        "  h2 = softmax_layer_pass(w2,h1,b2)\n",
        "  return h2\n",
        "  #print(H1.shape)\n",
        "  #print(H2.shape)\n",
        "\n"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ3EDtg1FocI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86a00dd6-82dd-4740-bee7-1c0172875704"
      },
      "source": [
        "# from lecture (VectorizedLogistiRegression, page 50) March 9th.... \n",
        "# We have a set of trainning of 60000 rows (instances)\n",
        "# and 748 columns ( Features)\n",
        "# Our X vector has 784 rows (as fetarues) and 60000 columns (instances)\n",
        "X = tf.transpose(tr_x)\n",
        "X = tf.cast(X, tf.float32)\n",
        "print(\"X shape is {}\".format( X.shape))\n",
        "# Let's create a vector W1 with the number of weights (784) for each neuron in one row\n",
        "# so, it means 200 rows and 784 columns ( Features) for layer 1\n",
        "W1 = tf.Variable(tf.random.normal([200,784], mean=0.0, stddev=0.05))\n",
        "#The bias for the first layer of 200 neurons will be a vecoter of 200 rows (1 per neuron)\n",
        "# and 1 column\n",
        "B1 = tf.Variable(tf.random.uniform(shape=[200,1],dtype=tf.float32))\n",
        "\n",
        "\n",
        "print (\"W1 shape is{}\".format(W1.shape))\n",
        "print (\"B1 shape is{}\".format(B1.shape))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape is (784, 60000)\n",
            "W1 shape is(200, 784)\n",
            "B1 shape is(200, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRf3fRXNqhyL",
        "outputId": "e3ce1811-4bff-43cd-91ec-9f2a28b2f02f"
      },
      "source": [
        "#The second layer:\n",
        "# W2 has 10 rows (as it has 10 possible classes)\n",
        "W2 = tf.Variable(tf.random.uniform(shape=[10,200], dtype=tf.float32, minval=-0.1, maxval=0.1))\n",
        "B2 = tf.Variable(tf.random.uniform(shape=[10,1], dtype=tf.float32))\n",
        "print (\"W2 shape is{}\".format(W2.shape))\n",
        "print (\"B2 shape is{}\".format(B2.shape))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W2 shape is(10, 200)\n",
            "B2 shape is(10, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC6WZg8RFp19"
      },
      "source": [
        "Question 1, section a. Question1_1_1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fcn1nNpPu-VQ"
      },
      "source": [
        "learning_rate=0.01\n",
        "iterations=1\n",
        "adam_optimizer=tf.keras.optimizers.Adam()\n"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "cKIsW-7RvF-p",
        "outputId": "fbffbd0d-797a-4606-99f6-76713c01cbd3"
      },
      "source": [
        "# Rgression loop\n",
        "for i in range(iterations):\n",
        "  H = forward_pass(W1,W2,B1,B2,X)\n",
        "  H1 = relu_layer_pass(W1, X, B1)\n",
        "  H2 = softmax_layer_pass(W2, H1, B2)\n",
        "  print(\"A column of H2 {}\".format(H2[:,7489]))\n",
        "  print(\"Sum of first column of H2 {}\".format(tf.reduce_sum(H2[:,7489])))\n",
        "\n",
        "  E = tf.subtract(H,tr_y)\n",
        "  \n",
        "  print(\"E shape is {}\".format(E.shape))\n",
        "  print(\"W1 shape is {}\".format(W1.shape))\n",
        "  #print(\"H shape is {}\".format(H.shape))\n",
        "  print(\"H1 shape is {}\".format(H1.shape))\n",
        "  print(\"H2 shape is {}\".format(H2.shape))\n",
        "  print(\"X shape is {}\".format(X.shape))\n",
        "  db2, dl2 = loss_function(E, H1)\n",
        "  print(\"dl2 shape is {}\".format(dl2.shape))\n",
        "  B2 = tf.subtract(B2, tf.scalar_mul(learning_rate, db2))\n",
        "  secondpart2 = tf.transpose(tf.scalar_mul(learning_rate, dl2)) #TODO WTF??? WHY?\n",
        "  print(\"secondpart shape is {}\".format(secondpart2.shape))\n",
        "  W2 = tf.subtract(W2,secondpart2)\n",
        "  db1, dl1 = loss_function(E, X)\n",
        "  print(\"dl1 shape is {}\".format(dl1.shape))\n",
        "  B1 = tf.subtract(B1, tf.scalar_mul(learning_rate, db1))\n",
        "  secondpart1 = tf.transpose(tf.scalar_mul(learning_rate, dl1)) #TODO WTF??? WHY?\n",
        "  print(\"secondpart shape is {}\".format(secondpart1.shape))\n",
        "  W1 = tf.subtract(W1,secondpart1)\n",
        "  print()\n",
        "print(\"Done\") \n",
        "    "
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A column of H2 [0.22727203 0.07542747 0.0402059  0.08087146 0.23783547 0.03462408\n",
            " 0.07598529 0.04618661 0.04790181 0.13368995]\n",
            "Sum of first column of H2 1.0000001192092896\n",
            "E shape is (10, 60000)\n",
            "W1 shape is (200, 784)\n",
            "H1 shape is (200, 60000)\n",
            "H2 shape is (10, 60000)\n",
            "X shape is (784, 60000)\n",
            "shape of x (200, 60000), shape of E transposed (60000, 10)\n",
            "dl2 shape is (200, 10)\n",
            "secondpart shape is (10, 200)\n",
            "shape of x (784, 60000), shape of E transposed (60000, 10)\n",
            "dl1 shape is (784, 10)\n",
            "secondpart shape is (10, 784)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-205d94420519>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0msecondpart1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#TODO WTF??? WHY?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"secondpart shape is {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecondpart1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0mW1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msecondpart1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36msubtract\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m   \"\"\"\n\u001b[0;32m--> 561\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m  10305\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10306\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10307\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10308\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10309\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [200,784] vs. [10,784] [Op:Sub]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU_kVxL341KF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5dc755d-62c5-42ae-b7d9-975e574e4ef3"
      },
      "source": [
        "#Unit tests\n",
        "xu = tf.Variable([[0.3,0.2,0.1],\n",
        "                  [0.7,0.6,0.5],\n",
        "                  [0.2,0.2,0.2]])\n",
        "big = tf.Variable([[6.0,8.0,10.0]], shape = [1,3])\n",
        "small = tf.Variable([5.0,7.0,9.0])\n",
        "E = tf.math.subtract(big, small)\n",
        "minus_E = tf.math.subtract(small, big)\n",
        "print(loss_function(E, xu)) # should be xu/3\n",
        "print(loss_function(minus_E, xu)) # should be -xu/3"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of x (3, 3), shape of E transposed (3, 1)\n",
            "(<tf.Tensor: shape=(), dtype=float32, numpy=3.0>, <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
            "array([[0.6],\n",
            "       [1.8],\n",
            "       [0.6]], dtype=float32)>)\n",
            "shape of x (3, 3), shape of E transposed (3, 1)\n",
            "(<tf.Tensor: shape=(), dtype=float32, numpy=-3.0>, <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
            "array([[-0.6],\n",
            "       [-1.8],\n",
            "       [-0.6]], dtype=float32)>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8I_oeqYZvUW"
      },
      "source": [
        "scalar = tf.Variable(3.0)\n",
        "vector = tf.Variable([3,6,9], dtype=tf.float32)\n",
        "prod = tf.scalar_mul(scalar, vector)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5jDdWo_YFqw",
        "outputId": "08979247-3c80-464f-aecc-463f2e5fdd9d"
      },
      "source": [
        "prod"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 9., 18., 27.], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-ZOQiUHRwKQ",
        "outputId": "0b611af0-8ed8-4a28-88df-6ac7c489a7e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# implementing  slide 31 of NeuralNetworks5\n",
        "A = tf.Variable([[6,-2, 3]], dtype = tf.float32)\n",
        "t = tf.math.exp(A)\n",
        "t"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[4.0342880e+02, 1.3533528e-01, 2.0085537e+01]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei3Iup8wUWLz",
        "outputId": "b1259e8c-8821-415c-f078-4abf72135979",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sum = tf.reduce_sum(t)\n",
        "sum"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=423.6497>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PhyMm4YUnO7",
        "outputId": "cd3841b1-b5fb-4354-9a6d-871061522aa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "htest = t/sum\n",
        "htest"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[9.5226979e-01, 3.1945092e-04, 4.7410719e-02]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up3qSY7qUwpz",
        "outputId": "c5539546-a573-4a16-ca82-6c0582df231b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "totalTest = tf.reduce_sum(htest)\n",
        "totalTest"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.99999994>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fEn-n0gVKh8",
        "outputId": "d3836a7f-913d-4249-c3ba-954553ddded5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Slide 36, calculate the loss function for softmax\n",
        "t_expe = tf.Variable([[1.0], [0.0], [0.0]], shape=[3,1])\n",
        "log_pr = -1*(tf.math.log(htest))\n",
        "log_pr"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.04890689, 8.048907  , 3.048907  ]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnRyVCQZgBf3",
        "outputId": "b679b0a8-4483-4a2d-9e17-17889d4efb8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "res = tf.matmul(log_pr,t_expe)\n",
        "res"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.04890689]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyYHhNjvhQoF",
        "outputId": "93c5f56a-0d19-4684-dc5d-1bc3d773d3f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "triv = tf.Variable(tf.exp(1.0), dtype=tf.float32)\n",
        "proof = tf.math.log(triv)\n",
        "proof"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.99999994>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1SPMJJGl1lv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}